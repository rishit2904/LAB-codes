{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6 - Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4766126511209428\n",
      "Epoch 2, Loss: 0.34617566657282395\n",
      "Epoch 3, Loss: 0.31446842814305187\n",
      "Epoch 4, Loss: 0.29103892898635825\n",
      "Epoch 5, Loss: 0.2779804645602637\n",
      "Model weights saved to 'model_weights.pth'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "def MNIST_CNN():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        nn.Dropout(0.25),\n",
    "        nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        nn.Dropout(0.25),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(64 * 7 * 7, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(128, 10)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = MNIST_CNN().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "torch.save(model.state_dict(), './model_weights.pth')\n",
    "print(\"Model weights saved to 'model_weights.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded successfully!\n",
      "Epoch 1, Loss: 0.25409031931017\n",
      "Epoch 2, Loss: 0.25084169825781255\n",
      "Epoch 3, Loss: 0.25027809344502144\n",
      "Epoch 4, Loss: 0.2522076402288447\n",
      "Epoch 5, Loss: 0.2532311462556947\n",
      "Epoch 6, Loss: 0.24756420165030305\n",
      "Epoch 7, Loss: 0.25057930184770494\n",
      "Epoch 8, Loss: 0.25061074967609287\n",
      "Epoch 9, Loss: 0.2491190151603364\n",
      "Epoch 10, Loss: 0.25017006182943835\n",
      "Accuracy on the test images: 91.14%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# Define transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define CNN Model (same as before)\n",
    "def MNIST_CNN():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        nn.Dropout(0.25),\n",
    "        nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        nn.Dropout(0.25),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(64 * 7 * 7, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(128, 10)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Initialize the model and move it to the appropriate device\n",
    "model = MNIST_CNN().to(device)\n",
    "\n",
    "# Check if saved weights exist\n",
    "if os.path.exists('./model_weights.pth'):\n",
    "    model.load_state_dict(torch.load('./model_weights.pth'))\n",
    "    print(\"Model weights loaded successfully!\")\n",
    "else:\n",
    "    print(\"No pre-trained model found, please train the model first.\")\n",
    "    # Optionally: You could exit here or train the model if weights don't exist.\n",
    "    exit()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune only the last layer\n",
    "optimizer = optim.Adam(model[-1].parameters(), lr=0.001)  # Fine-tuning only last layer\n",
    "\n",
    "# Fine-tuning loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Testing the model after fine-tuning\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for data in test_loader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on the test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/main\" to /home/student/.cache/torch/hub/main.zip\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /home/student/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 0: 0.2305 Acc: 0.0075\n",
      "Valid Acc at epoch 0: 0.9660\n",
      "Train Loss at epoch 1: 0.0505 Acc: 0.0080\n",
      "Valid Acc at epoch 1: 0.9550\n",
      "Train Loss at epoch 2: 0.0413 Acc: 0.0080\n",
      "Valid Acc at epoch 2: 0.9570\n",
      "Train Loss at epoch 3: 0.0109 Acc: 0.0080\n",
      "Valid Acc at epoch 3: 0.9670\n",
      "Train Loss at epoch 4: 0.0063 Acc: 0.0080\n",
      "Valid Acc at epoch 4: 0.9590\n",
      "Accuracy of the network on the 1000 test images: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dir = './cats_and_dogs_filtered/train'\n",
    "valid_dir = './cats_and_dogs_filtered/validation'\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    'train': datasets.ImageFolder(train_dir, data_transforms['train']),\n",
    "    'valid': datasets.ImageFolder(valid_dir, data_transforms['valid'])\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(image_datasets['train'], batch_size=32, shuffle=True),\n",
    "    'valid': DataLoader(image_datasets['valid'], batch_size=32, shuffle=False)\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "model_ft = torch.hub.load('pytorch/vision', 'alexnet', weights='IMAGENET1K_V1')\n",
    "\n",
    "num_ftrs = model_ft.classifier[6].in_features\n",
    "model_ft.classifier[6] = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, dataloaders, dataset_sizes, num_epochs=10, device='cpu'):\n",
    "   for e in range(num_epochs):\n",
    "\n",
    "       model.train()\n",
    "       running_loss = 0.0\n",
    "       running_corrects = 0\n",
    "\n",
    "       for inputs, labels in dataloaders['train']:\n",
    "           inputs = inputs.to(device)\n",
    "           labels = labels.to(device)\n",
    "           optimizer.zero_grad()\n",
    "\n",
    "           outputs = model(inputs)\n",
    "           _, preds = torch.max(outputs, 1)\n",
    "           loss = criterion(outputs, labels)\n",
    "\n",
    "           loss.backward()\n",
    "           optimizer.step()\n",
    "\n",
    "           running_loss += loss.item() * inputs.size(0)\n",
    "       running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "       train_loss = running_loss / dataset_sizes['train']\n",
    "       train_acc = running_corrects.double() / dataset_sizes['train']\n",
    "\n",
    "       print(f'Train Loss at epoch {e}: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
    "\n",
    "       model.eval()\n",
    "       running_corrects = 0\n",
    "\n",
    "       for inputs, labels in dataloaders['valid']:\n",
    "           inputs = inputs.to(device)\n",
    "           labels = labels.to(device)\n",
    "\n",
    "           with torch.no_grad():\n",
    "               outputs = model(inputs)\n",
    "               _, preds = torch.max(outputs, 1)\n",
    "               running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "       val_acc = running_corrects.double() / dataset_sizes['valid']\n",
    "\n",
    "       print(f'Valid Acc at epoch {e}: {val_acc:.4f}')\n",
    "\n",
    "   return model\n",
    "\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, dataloaders, dataset_sizes, num_epochs=5, device=device)\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(dataset_sizes['valid'], 100 * correct / total))\n",
    "\n",
    "\n",
    "evaluate_model(model_ft, dataloaders['valid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "Average Loss: 2.195639673580747, Accuracy: 38.24166666666667%\n",
      "Checkpoint saved at epoch 1\n",
      "EPOCH 2\n",
      "Average Loss: 1.5454540382316118, Accuracy: 62.055%\n",
      "Checkpoint saved at epoch 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "model = CNNClassifier().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "os.makedirs(\"./checkpoints\", exist_ok=True)\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'EPOCH {epoch + 1}')\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_preds += labels.size(0)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100 * correct_preds / total_preds\n",
    "    print(f'Average Loss: {avg_loss}, Accuracy: {accuracy}%')\n",
    "\n",
    "    checkpoint = {\n",
    "        \"last_loss\": avg_loss,\n",
    "        \"last_accuracy\": accuracy, \n",
    "        \"last_epoch\": epoch + 1,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, \"./checkpoints/checkpoint.pt\")\n",
    "    print(f'Checkpoint saved at epoch {epoch + 1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 2 with last loss 1.5454540382316118 and accuracy 62.055%\n",
      "EPOCH 3\n",
      "Average Loss: 0.8642180255735352, Accuracy: 72.69166666666666%\n",
      "Checkpoint saved at epoch 3\n",
      "EPOCH 4\n",
      "Average Loss: 0.6956186698062587, Accuracy: 75.47166666666666%\n",
      "Checkpoint saved at epoch 4\n",
      "EPOCH 5\n",
      "Average Loss: 0.6406074377583034, Accuracy: 76.94333333333333%\n",
      "Checkpoint saved at epoch 5\n",
      "EPOCH 6\n",
      "Average Loss: 0.6070120503653341, Accuracy: 78.01333333333334%\n",
      "Checkpoint saved at epoch 6\n",
      "EPOCH 7\n",
      "Average Loss: 0.5828836154518351, Accuracy: 79.01166666666667%\n",
      "Checkpoint saved at epoch 7\n",
      "EPOCH 8\n",
      "Average Loss: 0.5630985355771172, Accuracy: 79.65166666666667%\n",
      "Checkpoint saved at epoch 8\n",
      "EPOCH 9\n",
      "Average Loss: 0.5457978423661006, Accuracy: 80.22166666666666%\n",
      "Checkpoint saved at epoch 9\n",
      "EPOCH 10\n",
      "Average Loss: 0.5311935498897455, Accuracy: 80.80666666666667%\n",
      "Checkpoint saved at epoch 10\n"
     ]
    }
   ],
   "source": [
    "model = CNNClassifier().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "checkpoint = torch.load(\"./checkpoints/checkpoint.pt\")\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "\n",
    "last_loss = checkpoint[\"last_loss\"]\n",
    "last_accuracy = checkpoint[\"last_accuracy\"]\n",
    "last_epoch = checkpoint[\"last_epoch\"]\n",
    "print(f'Resuming training from epoch {last_epoch} with last loss {last_loss} and accuracy {last_accuracy}%')\n",
    "\n",
    "NEW_EPOCHS = 10\n",
    "\n",
    "for epoch in range(last_epoch, NEW_EPOCHS):\n",
    "    print(f'EPOCH {epoch + 1}')\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_preds += labels.size(0)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100 * correct_preds / total_preds\n",
    "    print(f'Average Loss: {avg_loss}, Accuracy: {accuracy}%')\n",
    "\n",
    "    checkpoint = {\n",
    "        \"last_loss\": avg_loss,\n",
    "        \"last_accuracy\": accuracy,  # Saving the accuracy\n",
    "        \"last_epoch\": epoch + 1,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, \"./checkpoints/checkpoint.pt\")\n",
    "    print(f'Checkpoint saved at epoch {epoch + 1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "dllab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
